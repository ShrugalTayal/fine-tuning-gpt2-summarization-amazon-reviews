{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Summarization using GPT2\n",
    "\n",
    "## Author: \n",
    "ShrugalTayal (shrugal20408@iiitd.ac.in)\n",
    "\n",
    "## Introduction\n",
    "This notebook guides through fine-tuning Hugging Face's GPT-2 model on Amazon Fine Food Reviews dataset for summarization.\n",
    "\n",
    "Through this exploration, we seek to improve retrieval accuracy and user experience in information retrieval applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Amazon Fine Food Reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../res/Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the column names of the DataFrame\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>ADT0SRK1MGOEU</td>\n",
       "      <td>Twoapennything</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1342051200</td>\n",
       "      <td>Nice Taffy</td>\n",
       "      <td>I got a wild hair for taffy and ordered this f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1SP2KVKFXXRU1</td>\n",
       "      <td>David C. Sullivan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1340150400</td>\n",
       "      <td>Great!  Just as good as the expensive brands!</td>\n",
       "      <td>This saltwater taffy had great flavors and was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A3JRGQVEQN31IQ</td>\n",
       "      <td>Pamela G. Williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1336003200</td>\n",
       "      <td>Wonderful, tasty taffy</td>\n",
       "      <td>This taffy is so good.  It is very soft and ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>B000E7L2R4</td>\n",
       "      <td>A1MZYO9TZK0BBI</td>\n",
       "      <td>R. James</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1322006400</td>\n",
       "      <td>Yay Barley</td>\n",
       "      <td>Right now I'm mostly just sprouting this so my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>B00171APVA</td>\n",
       "      <td>A21BT40VZCCYT4</td>\n",
       "      <td>Carol A. Reed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>Healthy Dog Food</td>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "5   6  B006K2ZZ7K   ADT0SRK1MGOEU                   Twoapennything   \n",
       "6   7  B006K2ZZ7K  A1SP2KVKFXXRU1                David C. Sullivan   \n",
       "7   8  B006K2ZZ7K  A3JRGQVEQN31IQ               Pamela G. Williams   \n",
       "8   9  B000E7L2R4  A1MZYO9TZK0BBI                         R. James   \n",
       "9  10  B00171APVA  A21BT40VZCCYT4                    Carol A. Reed   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "5                     0                       0      4  1342051200   \n",
       "6                     0                       0      5  1340150400   \n",
       "7                     0                       0      5  1336003200   \n",
       "8                     1                       1      5  1322006400   \n",
       "9                     0                       0      5  1351209600   \n",
       "\n",
       "                                         Summary  \\\n",
       "0                          Good Quality Dog Food   \n",
       "1                              Not as Advertised   \n",
       "2                          \"Delight\" says it all   \n",
       "3                                 Cough Medicine   \n",
       "4                                    Great taffy   \n",
       "5                                     Nice Taffy   \n",
       "6  Great!  Just as good as the expensive brands!   \n",
       "7                         Wonderful, tasty taffy   \n",
       "8                                     Yay Barley   \n",
       "9                               Healthy Dog Food   \n",
       "\n",
       "                                                Text  \n",
       "0  I have bought several of the Vitality canned d...  \n",
       "1  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  This is a confection that has been around a fe...  \n",
       "3  If you are looking for the secret ingredient i...  \n",
       "4  Great taffy at a great price.  There was a wid...  \n",
       "5  I got a wild hair for taffy and ordered this f...  \n",
       "6  This saltwater taffy had great flavors and was...  \n",
       "7  This taffy is so good.  It is very soft and ch...  \n",
       "8  Right now I'm mostly just sprouting this so my...  \n",
       "9  This is a very healthy dog food. Good for thei...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the first 10 rows of the DataFrame\n",
    "# This line returns a new DataFrame containing the first 10 rows of the original DataFrame\n",
    "# It's useful for quickly inspecting the structure and content of the dataset\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in each column:\n",
      "Id                         0\n",
      "ProductId                  0\n",
      "UserId                     0\n",
      "ProfileName               26\n",
      "HelpfulnessNumerator       0\n",
      "HelpfulnessDenominator     0\n",
      "Score                      0\n",
      "Time                       0\n",
      "Summary                   27\n",
      "Text                       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Find NaN values in each column\n",
    "nan_values_per_column = data.isna().sum()\n",
    "\n",
    "print(\"NaN values in each column:\")\n",
    "print(nan_values_per_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame before dropping rows with NaN values: (568454, 10)\n",
      "Shape of DataFrame after dropping rows with NaN values: (568401, 10)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the DataFrame before dropping NaN values\n",
    "print(\"Shape of DataFrame before dropping rows with NaN values:\", data.shape)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "# Print the shape of the DataFrame after dropping NaN values\n",
    "print(\"Shape of DataFrame after dropping rows with NaN values:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sampled DataFrame: (142100, 10)\n"
     ]
    }
   ],
   "source": [
    "# Take only 25% of the data randomly\n",
    "data_sampled = data.sample(frac=0.25, random_state=42)  # Adjust the random_state as needed for reproducibility\n",
    "\n",
    "# Print the shape of the sampled DataFrame\n",
    "print(\"Shape of sampled DataFrame:\", data_sampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing on the ‘Text’ and ‘Summary’ column from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and numbers using regex\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join tokens back into text\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess the 'Text' column\n",
    "data_sampled['cleaned_text'] = data_sampled['Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess the 'Summary' column\n",
    "data_sampled['cleaned_summary'] = data_sampled['Summary'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18828</th>\n",
       "      <td>avocado oil beat olive oil dressing anything v...</td>\n",
       "      <td>avocado oil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363857</th>\n",
       "      <td>new year resolution season bought protein plus...</td>\n",
       "      <td>healthy tastesoky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342609</th>\n",
       "      <td>pretty sad got peach indeed delicious stated c...</td>\n",
       "      <td>peach china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62213</th>\n",
       "      <td>find people north america brew tea long period...</td>\n",
       "      <td>perhaps brewing wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467133</th>\n",
       "      <td>love recycled work well storebought product ce...</td>\n",
       "      <td>work great</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             cleaned_text  \\\n",
       "18828   avocado oil beat olive oil dressing anything v...   \n",
       "363857  new year resolution season bought protein plus...   \n",
       "342609  pretty sad got peach indeed delicious stated c...   \n",
       "62213   find people north america brew tea long period...   \n",
       "467133  love recycled work well storebought product ce...   \n",
       "\n",
       "              cleaned_summary  \n",
       "18828             avocado oil  \n",
       "363857      healthy tastesoky  \n",
       "342609            peach china  \n",
       "62213   perhaps brewing wrong  \n",
       "467133             work great  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_sampled[['cleaned_text', 'cleaned_summary']]\n",
    "\n",
    "# Verify the resulting DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "1. Initialize GPT-2 Tokenizer and Model\n",
    "2. Data Splitting: 75:25 for training and testing.\n",
    "3. Custom Dataset Class for data prep.\n",
    "4. Fine-Tune GPT-2 on review dataset.\n",
    "5. Hyperparameter Tuning for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0337f564c1247398b1566ecf08b38d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f1db1853d04637aaa4db56f35b1cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/889 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.584134101867676, 'eval_runtime': 1297.9282, 'eval_samples_per_second': 2.737, 'eval_steps_per_second': 0.685, 'epoch': 1.0}\n",
      "{'train_runtime': 21533.9564, 'train_samples_per_second': 0.495, 'train_steps_per_second': 0.124, 'train_loss': 2.555314112335835, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('fine_tuned_gpt2_model\\\\tokenizer_config.json',\n",
       " 'fine_tuned_gpt2_model\\\\special_tokens_map.json',\n",
       " 'fine_tuned_gpt2_model\\\\vocab.json',\n",
       " 'fine_tuned_gpt2_model\\\\merges.txt',\n",
       " 'fine_tuned_gpt2_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Add padding token to the tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "class AmazonReviewDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['cleaned_text']\n",
    "        summary = self.data.iloc[idx]['cleaned_summary']\n",
    "\n",
    "        input_text = summary + ' ' + text\n",
    "        inputs = self.tokenizer(input_text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        labels = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')['input_ids']\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'labels': labels.flatten()\n",
    "        }\n",
    "\n",
    "\n",
    "# Create training and testing datasets\n",
    "train_dataset = AmazonReviewDataset(train_df, tokenizer)\n",
    "test_dataset = AmazonReviewDataset(test_df, tokenizer)\n",
    "\n",
    "\n",
    "class DeleteCheckpointCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if args.output_dir is not None:\n",
    "            # Delete checkpoints and logs after each step\n",
    "            control.should_save = False\n",
    "            control.should_log = False\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Directory to save model checkpoints and logs\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Define Trainer object with DeleteCheckpointCallback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    callbacks=[DeleteCheckpointCallback()]\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the trained model and tokenizer\n",
    "model.save_pretrained(\"fine_tuned_gpt2_model\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_gpt2_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Compute ROUGE scores on test set to assess model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: The Fender CD-60S Dreadnought Acoustic Guitar is a great instrument for beginners. It has a solid construction, produces a rich sound, and feels comfortable to play. However, some users have reported issues with the tuning stability.\n",
      "ROUGE-1: Precision: 0.17647058823529413 Recall: 0.75 F1-Score: 0.2857142826303855\n",
      "ROUGE-2: Precision: 0.05714285714285714 Recall: 0.2857142857142857 F1-Score: 0.09523809246031753\n",
      "ROUGE-L: Precision: 0.14705882352941177 Recall: 0.625 F1-Score: 0.2380952350113379\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from rouge import Rouge\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"fine_tuned_gpt2_model\")\n",
    "\n",
    "# Load model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"fine_tuned_gpt2_model\")\n",
    "\n",
    "# Define review text\n",
    "review_text = \"The Fender CD-60S Dreadnought Acoustic Guitar is a great instrument for beginners. It has a solid construction, produces a rich sound, and feels comfortable to play. However, some users have reported issues with the tuning stability.\"\n",
    "# review_text = input(\"Given Review Text: \")\n",
    "\n",
    "# Tokenize the review text\n",
    "inputs = tokenizer(review_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate summary\n",
    "generated_summary_ids = model.generate(inputs.input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
    "generated_summary = tokenizer.decode(generated_summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print generated summary\n",
    "print(\"Generated Summary:\", generated_summary)\n",
    "\n",
    "# Define actual summary\n",
    "actual_summary = \"Good for beginners but has tuning stability issues.\"\n",
    "# actual_summary = input(\"Given Summary :\")\n",
    "\n",
    "# Compute ROUGE scores\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(generated_summary, actual_summary, avg=True)\n",
    "\n",
    "# Print ROUGE scores\n",
    "print(\"ROUGE-1: Precision:\", scores[\"rouge-1\"][\"p\"], \"Recall:\", scores[\"rouge-1\"][\"r\"], \"F1-Score:\", scores[\"rouge-1\"][\"f\"])\n",
    "print(\"ROUGE-2: Precision:\", scores[\"rouge-2\"][\"p\"], \"Recall:\", scores[\"rouge-2\"][\"r\"], \"F1-Score:\", scores[\"rouge-2\"][\"f\"])\n",
    "print(\"ROUGE-L: Precision:\", scores[\"rouge-l\"][\"p\"], \"Recall:\", scores[\"rouge-l\"][\"r\"], \"F1-Score:\", scores[\"rouge-l\"][\"f\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
